from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.sensors.external_task import ExternalTaskSensor
from datetime import datetime, timedelta
import logging
import pandas as pd
import json
from tempfile import NamedTemporaryFile
import os

from utils.slack_alert import SlackAlert
from utils.constants import TRANSFORMED_MATCH_BUCKET, TRANSFORMED_MASTERY_BUCKET


def _merge_parquet_files(parquet_files):
    merged_dataframe = pd.DataFrame()
    for idx, parquet_file in enumerate(parquet_files):
        try:
            dataframe = pd.read_parquet(parquet_file)
            logging.info(
                f"ğŸš€Successfully read {parquet_file}. DataFrame shape: {dataframe.shape}"
            )
            logging.info(f"DataFrame columns: {dataframe.columns}")
            merged_dataframe = pd.concat([merged_dataframe, dataframe])
        except Exception as e:
            logging.error(
                f"ğŸš¨Failed to read or merge a dataframe at index {idx} due to {e}"
            )
    return merged_dataframe


def _extract_match_details(row):
    import os

    def _extract_values(participants, key):
        return [participant[key] for participant in participants]

    def _extract_bans(teams):
        bans_list = []
        for team in teams:
            if "bans" in team:
                bans = [ban_info["championId"] for ban_info in team["bans"]]
            else:
                bans = [-1] * 5  # ban ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° -1ë¡œ ì±„ì›ë‹ˆë‹¤.
            bans_list.extend(bans)
        return bans_list

    if isinstance(row["match_details"], str):  # match_detailsê°€ ë¬¸ìì—´ í˜•íƒœì¸ ê²½ìš°
        details = json.loads(row["match_details"])
    elif isinstance(row["match_details"], dict):  # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
        details = row["match_details"]
    else:
        print("Unknown data type for match_details")
        return

    if "info" in details:
        participants = details["info"]["participants"]
        bans = _extract_bans(details["info"]["teams"])

        # load champion mapping data
        script_path = os.path.dirname(os.path.abspath(__file__))
        json_file_path = os.path.join(script_path, "utils", "champion_dictionary.json")
        with open(json_file_path, "r") as f:
            champion_dict = json.load(f)

        patch = ".".join(str(details["info"]["gameVersion"]).split(".")[0:2])
        banned_champion_names = [
            champion_dict.get(str(ban_id), "Unknown") for ban_id in bans
        ]

        return pd.Series(
            {
                "team_id": _extract_values(participants, "teamId"),
                "position": _extract_values(participants, "teamPosition"),
                "kills": _extract_values(participants, "kills"),
                "deaths": _extract_values(participants, "deaths"),
                "assists": _extract_values(participants, "assists"),
                "win": _extract_values(participants, "win"),
                "champion_name": _extract_values(participants, "championName"),
                "champion_id": _extract_values(participants, "championId"),
                "banned_champion_id": bans,  # ë°´ ì •ë³´ ì¶”ê°€
                "banned_champion_name": banned_champion_names,  # ë°´ëœ ì±”í”¼ì–¸ ì´ë¦„ ì¶”ê°€
                "patch": patch,
                "tier": row["tier"],
                "match_id": row["match_id"],
            }
        )
    elif "status" in details:
        print(f"Skipping row due to status: {details['status']}")
        return pd.Series()
    else:
        print(f"Skipping row due to missing 'info' and 'status' keys: {row}")
        return pd.Series()


def _expand_row(row):
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ë“¤
    list_columns = [
        "team_id",
        "position",
        "kills",
        "deaths",
        "assists",
        "win",
        "champion_name",
        "champion_id",
        "banned_champion_name",
        "banned_champion_id",
    ]

    # Scalar ê°’ìœ¼ë¡œ ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ë“¤
    scalar_columns = ["patch", "tier", "match_id"]

    # ìƒˆë¡œìš´ rowë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    new_rows = []

    # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ì€ ê¸¸ì´ê°€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •)
    lengths = [len(row[col]) for col in list_columns]

    # ê¸¸ì´ê°€ ëª¨ë‘ ë™ì¼í•œì§€ í™•ì¸
    if len(set(lengths)) != 1:
        print(f"Skipping row due to inconsistent list lengths: {lengths}")
        return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜

    n = lengths[0]  # ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´

    for i in range(n):
        new_row = {}
        for col in list_columns:
            new_row[col] = row[col][i]

        for col in scalar_columns:
            new_row[col] = row[col]

        new_rows.append(new_row)

    return pd.DataFrame(new_rows)


def _load_mastery_details(row, champion_dict):
    tmp_str = row["mastery_details"]
    try:
        tmp = json.loads(tmp_str) if tmp_str is not None else []
    except json.JSONDecodeError as e:
        logging.error(f"JSON decoding failed for summoner_id {row['summoner_id']}: {e}")
        return None

    data = {"id": row["summoner_id"]}

    for key in champion_dict.keys():
        if key != "id":
            data[key] = 0

    for champion in tmp:
        if "championId" in champion and "championPoints" in champion:
            champion_id = champion["championId"]
            champion_points = champion["championPoints"]
            if str(champion_id) in data:
                data[str(champion_id)] = champion_points

    return data


def _create_total_dataframe(mastery_df, champion_dict):
    data_list = []
    for _, row in mastery_df.iterrows():
        data = _load_mastery_details(row, champion_dict)
        if data is not None:
            data_list.append(data)

    if not data_list:
        logging.error("No data processed. DataFrame will be empty.")
        return pd.DataFrame()

    total_df = pd.DataFrame(data_list)
    logging.info("ğŸ˜Successfully created the total DataFrame.")
    return total_df


with DAG(
    dag_id="transform_riot_data",
    schedule_interval=None,
    # schedule_interval=timedelta(days=14),
    start_date=datetime(2023, 8, 29),
    catchup=False,
) as dag:

    @task()
    def transform_match_data():
        from utils.common_util import download_from_s3, upload_to_s3

        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logging.info("ğŸ“¥ Downloading parquet files from S3...")
        parquet_files = download_from_s3("match")

        if parquet_files:
            logging.info(f"ğŸ” Found {len(parquet_files)} parquet files.")

            # íŒŒì¼ ë³‘í•©
            logging.info("ğŸ”„ Merging parquet files...")
            merged_dataframe = _merge_parquet_files(parquet_files)

            # ë°ì´í„° ë³€í™˜
            logging.info("ğŸ”„ Transforming data...")
            transformed_dataframe = merged_dataframe.apply(
                _extract_match_details, axis=1
            )
            logging.info(
                f"âœ”ï¸ Transformed dataframe shape: {transformed_dataframe.shape}"
            )

            # DataFrame í™•ì¥
            logging.info("ğŸ”„ Expanding DataFrame rows...")
            expanded_df_list = []
            for idx, row in transformed_dataframe.iterrows():
                try:
                    expanded_df = _expand_row(row)
                    expanded_df_list.append(expanded_df)
                except ValueError as ve:
                    logging.error(f"ğŸš¨ ValueError at row {idx}: {ve}")
                except Exception as e:
                    logging.error(f"ğŸš¨ Unexpected error at row {idx}: {e}")

            if not expanded_df_list:
                logging.error("ğŸš¨ No data to process after expanding rows.")
                return

            final_expanded_df = pd.concat(expanded_df_list, ignore_index=True)

            # S3ì— ì—…ë¡œë“œí•˜ê¸° ì „ì— ë°ì´í„° ë¶„í• 
            logging.info("ğŸ”„ Chunking data...")
            chunk_size = 50000  # ì ì ˆí•œ í¬ê¸°ë¡œ ì„¤ì •
            total_chunks = len(final_expanded_df) // chunk_size + 1

            for i in range(total_chunks):
                logging.info(f"ğŸ“¤ Uploading chunk {i + 1}/{total_chunks} to S3...")
                chunk_df = final_expanded_df.iloc[i * chunk_size : (i + 1) * chunk_size]

                try:
                    with NamedTemporaryFile(
                        suffix=".csv", delete=False
                    ) as temp_file:  # delete=Falseë¡œ ì„¤ì •
                        chunk_df.to_csv(temp_file.name, index=True)
                        # S3ì— ì—…ë¡œë“œ
                        upload_to_s3(
                            temp_file.name,
                            TRANSFORMED_MATCH_BUCKET,
                            f"transformed_match_data_chunk_{i}",
                            file_type="csv",
                        )
                except Exception as e:
                    logging.error(f"ğŸš¨ Error during file operation: {e}")
                finally:
                    if os.path.exists(temp_file.name):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        os.unlink(temp_file.name)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ

            logging.info("âœ”ï¸ Data upload complete.")

    @task()
    def transform_mastery_data():
        from utils.common_util import download_from_s3, upload_to_s3

        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logging.info("ğŸ“¥ Downloading parquet files from S3...")
        parquet_files = download_from_s3("mastery")

        # load champion mapping data
        script_path = os.path.dirname(os.path.abspath(__file__))
        json_file_path = os.path.join(script_path, "utils", "champion_dictionary.json")
        with open(json_file_path, "r") as f:
            champion_dict = json.load(f)

        if parquet_files:
            logging.info(f"ğŸ” Found {len(parquet_files)} parquet files.")

            # íŒŒì¼ ë³‘í•©
            logging.info("ğŸ”„ Merging parquet files...")
            merged_dataframe = _merge_parquet_files(parquet_files)

            # ë°ì´í„° ë³€í™˜
            logging.info("ğŸ”„ Transforming data...")
            transformed_dataframe = _create_total_dataframe(
                merged_dataframe, champion_dict
            )
            logging.info(
                f"âœ”ï¸ Transformed dataframe shape: {transformed_dataframe.shape}"
            )

            # S3ì— ì—…ë¡œë“œí•˜ê¸° ì „ì— ë°ì´í„° ë¶„í• 
            logging.info("ğŸ”„ Chunking data...")
            chunk_size = 50000  # ì ì ˆí•œ í¬ê¸°ë¡œ ì„¤ì •
            total_chunks = len(transformed_dataframe) // chunk_size + 1

            for i in range(total_chunks):
                logging.info(f"ğŸ“¤ Uploading chunk {i + 1}/{total_chunks} to S3...")
                chunk_df = transformed_dataframe.iloc[
                    i * chunk_size : (i + 1) * chunk_size
                ]

                try:
                    with NamedTemporaryFile(
                        suffix=".csv", delete=False
                    ) as temp_file:  # delete=Falseë¡œ ì„¤ì •
                        chunk_df.to_csv(temp_file.name, index=True)
                        # S3ì— ì—…ë¡œë“œ
                        upload_to_s3(
                            temp_file.name,
                            TRANSFORMED_MASTERY_BUCKET,
                            f"transformed_mastery_data_chunk_{i}",
                            file_type="csv",
                        )
                except Exception as e:
                    logging.error(f"ğŸš¨ Error during file operation: {e}")
                finally:
                    if os.path.exists(temp_file.name):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        os.unlink(temp_file.name)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ

            logging.info("âœ”ï¸ Data upload complete.")

    start = EmptyOperator(task_id="start")

    end = EmptyOperator(task_id="transform_end")

    slack_alert = SlackAlert(channel="#lulu-airflow-alert")

    wait_for_get_riot_api_dag = ExternalTaskSensor(
        task_id="wait_for_get_riot_api_dag",
        external_dag_id="get_riot_api",
        external_task_id="end",
        timeout=600,
        mode="poke",
        dag=dag,
    )

    transform_match_data = PythonOperator(
        task_id="transform_match_data",
        python_callable=transform_match_data,
        retries=3,  # ì‹¤íŒ¨í•œ ì‘ì—… 3íšŒ ì¬ì‹œë„
        retry_delay=timedelta(minutes=5),  # ê° ì¬ì‹œë„ ì‚¬ì´ì˜ 5ë¶„ ì§€ì—° ì‹œê°„
        execution_timeout=timedelta(minutes=60),  # ì‘ì—…ì´ 60ë¶„ ì´ˆê³¼í•˜ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
        on_success_callback=slack_alert.slack_success_alert,
        on_failure_callback=slack_alert.slack_failure_alert,
        dag=dag,
    )

    transform_mastery_data = PythonOperator(
        task_id="transform_mastery_data",
        python_callable=transform_mastery_data,
        retries=3,
        retry_delay=timedelta(minutes=5),
        execution_timeout=timedelta(minutes=60),
        on_success_callback=slack_alert.slack_success_alert,
        on_failure_callback=slack_alert.slack_failure_alert,
        dag=dag,
    )

    (
        start
        >> wait_for_get_riot_api_dag
        >> transform_match_data
        >> transform_mastery_data
        >> end
    )
