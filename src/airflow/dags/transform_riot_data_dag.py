from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.sensors.external_task import ExternalTaskSensor
from datetime import datetime, timedelta
import logging
import pandas as pd
import json
from tempfile import NamedTemporaryFile
import os
import gc

from utils.slack_alert import SlackAlert
from utils.constants import TRANSFORMED_MATCH_BUCKET, TRANSFORMED_MASTERY_BUCKET


def batch(iterable, n=1):
    from itertools import islice

    """
    iterableì„ í¬ê¸° nì˜ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    
    Parameters:
    - iterable (iterable): ì²­í¬ ë‚˜ëˆŒ iterable ê°ì²´
    - n (int): ê° ì²­í¬ì˜ í¬ê¸°
    
    Yields (list): ì›ë˜ì˜ iterableì—ì„œ í¬ê¸° nì˜ ì²­í¬
    
    * ìš°ë¦¬ ì½”ë“œì—ì„œëŠ” í¬ê¸° nì˜ parquet file ë¦¬ìŠ¤íŠ¸ë¼ê³  ë³´ë©´ ëœë‹¤.
    
    Examples:
    >>> list(batch([1, 2, 3, 4, 5], 2))
    [[1, 2], [3, 4], [5]]
    """
    it = iter(iterable)
    while True:
        chunk = list(islice(it, n))
        if not chunk:
            return
        yield chunk


def _merge_parquet_files(parquet_files):
    merged_dataframe = pd.DataFrame()
    for idx, parquet_file in enumerate(parquet_files):
        try:
            dataframe = pd.read_parquet(parquet_file)
            logging.info(
                f"ğŸš€Successfully read {parquet_file}. DataFrame shape: {dataframe.shape}"
            )
            logging.info(f"DataFrame columns: {dataframe.columns}")
            merged_dataframe = pd.concat([merged_dataframe, dataframe])
        except Exception as e:
            logging.error(
                f"ğŸš¨Failed to read or merge a dataframe at index {idx} due to {e}"
            )
    return merged_dataframe


def _extract_match_details(row):
    import os

    def _extract_values(participants, key):
        return [participant[key] for participant in participants]

    def _extract_bans(teams):
        bans_list = []
        for team in teams:
            if "bans" in team:
                bans = [ban_info["championId"] for ban_info in team["bans"]]
            else:
                bans = [-1] * 5  # ban ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° -1ë¡œ ì±„ì›ë‹ˆë‹¤.
            bans_list.extend(bans)
        return bans_list

    if isinstance(row["match_details"], str):  # match_detailsê°€ ë¬¸ìì—´ í˜•íƒœì¸ ê²½ìš°
        details = json.loads(row["match_details"])
    elif isinstance(row["match_details"], dict):  # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
        details = row["match_details"]
    else:
        print("Unknown data type for match_details")
        return

    if "info" in details:
        queue_id = str(details["info"]["queueId"])

        if queue_id in ["420", "4"]:
            participants = details["info"]["participants"]
            bans = _extract_bans(details["info"]["teams"])

            # load champion mapping data
            script_path = os.path.dirname(os.path.abspath(__file__))
            json_file_path = os.path.join(
                script_path, "utils", "champion_dictionary.json"
            )
            with open(json_file_path, "r") as f:
                champion_dict = json.load(f)

            patch = ".".join(str(details["info"]["gameVersion"]).split(".")[0:2])
            banned_champion_names = [
                champion_dict.get(str(ban_id), "Unknown") for ban_id in bans
            ]

            return pd.Series(
                {
                    "team_id": _extract_values(participants, "teamId"),
                    "position": _extract_values(participants, "teamPosition"),
                    "kills": _extract_values(participants, "kills"),
                    "deaths": _extract_values(participants, "deaths"),
                    "assists": _extract_values(participants, "assists"),
                    "win": _extract_values(participants, "win"),
                    "champion_name": _extract_values(participants, "championName"),
                    "champion_id": _extract_values(participants, "championId"),
                    "banned_champion_id": bans,  # ë°´ ì •ë³´ ì¶”ê°€
                    "banned_champion_name": banned_champion_names,  # ë°´ëœ ì±”í”¼ì–¸ ì´ë¦„ ì¶”ê°€
                    "patch": patch,
                    "tier": row["tier"],
                    "match_id": row["match_id"],
                }
            )
    elif "status" in details:
        print(f"Skipping row due to status: {details['status']}")
        return pd.Series()
    else:
        print(f"Skipping row due to missing 'info' and 'status' keys: {row}")
        return pd.Series()


def _expand_row(row):
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ë“¤
    list_columns = [
        "team_id",
        "position",
        "kills",
        "deaths",
        "assists",
        "win",
        "champion_name",
        "champion_id",
        "banned_champion_name",
        "banned_champion_id",
    ]

    # Scalar ê°’ìœ¼ë¡œ ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ë“¤
    scalar_columns = ["patch", "tier", "match_id"]

    # ìƒˆë¡œìš´ rowë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    new_rows = []

    # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ì€ ê¸¸ì´ê°€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •)
    lengths = [len(row[col]) for col in list_columns]

    # ê¸¸ì´ê°€ ëª¨ë‘ ë™ì¼í•œì§€ í™•ì¸
    if len(set(lengths)) != 1:
        print(f"Skipping row due to inconsistent list lengths: {lengths}")
        return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜

    n = lengths[0]  # ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´

    for i in range(n):
        new_row = {}
        for col in list_columns:
            new_row[col] = row[col][i]

        for col in scalar_columns:
            new_row[col] = row[col]

        new_rows.append(new_row)

    return pd.DataFrame(new_rows)


def _load_mastery_details(row, champion_dict):
    tmp_str = row["mastery_details"]

    try:
        tmp = json.loads(tmp_str) if tmp_str is not None else []
    except json.JSONDecodeError as e:
        logging.error(f"JSON decoding failed for summoner_id {row['summoner_id']}: {e}")
        return None

    data = {"id": row["summoner_id"]}

    for key in champion_dict.keys():
        if key != "id":
            data[key] = 0

    for champion in tmp:
        if "championId" in champion and "championPoints" in champion:
            champion_id = champion["championId"]
            champion_points = champion["championPoints"]
            if str(champion_id) in data:
                data[str(champion_id)] = champion_points

    return data


def _create_total_dataframe(mastery_df, champion_dict):
    data_list = []
    for _, row in mastery_df.iterrows():
        data = _load_mastery_details(row, champion_dict)
        if data is not None:
            data_list.append(data)

    if not data_list:
        logging.error("No data processed. DataFrame will be empty.")
        return pd.DataFrame()

    total_df = pd.DataFrame(data_list)
    logging.info("ğŸ˜Successfully created the total DataFrame.")
    return total_df


with DAG(
    dag_id="transform_riot_data",
    schedule_interval=None,
    # schedule_interval=timedelta(days=14),
    start_date=datetime(2023, 8, 29),
    catchup=False,
) as dag:

    @task()
    def transform_match_data():
        from utils.common_util import download_from_s3, upload_to_s3

        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logging.info("ğŸ“¥ Downloading parquet files from S3...")
        parquet_files = download_from_s3("match")

        if parquet_files:
            logging.info(f"ğŸ” Found {len(parquet_files)} parquet files.")

            for batch_num, batch_files in enumerate(batch(parquet_files, 10)):
                logging.info(f"ğŸ”„ Merging parquet batch {batch_num + 1}...")
                merged_dataframe = _merge_parquet_files(batch_files)

                # ë°ì´í„° ë³€í™˜
                logging.info("ğŸ”„ Transforming data...")
                transformed_dataframe = merged_dataframe.apply(
                    _extract_match_details, axis=1
                )
                logging.info(
                    f"âœ”ï¸ Transformed dataframe shape: {transformed_dataframe.shape}"
                )

                # DataFrame í™•ì¥
                logging.info("ğŸ”„ Expanding DataFrame rows...")
                expanded_df_list = []
                if type(transformed_dataframe) == pd.core.frame.DataFrame:
                    for idx, row in transformed_dataframe.iterrows():
                        try:
                            expanded_df = _expand_row(row)
                            expanded_df_list.append(expanded_df)
                        except ValueError as ve:
                            logging.error(f"ğŸš¨ ValueError at row {idx}: {ve}")
                        except Exception as e:
                            logging.error(f"ğŸš¨ Unexpected error at row {idx}: {e}")

                if not expanded_df_list:
                    logging.error("ğŸš¨ No data to process after expanding rows.")
                    continue

                final_expanded_df = pd.concat(expanded_df_list, ignore_index=True)

                # ì—¬ê¸°ì„œ S3ì— ì—…ë¡œë“œ -> ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ batch_numì„ ì´ìš©í•´ì„œ íŒŒì¼ ì´ë¦„ êµ¬ë¶„
                logging.info(f"ğŸ“¤ Uploading batch {batch_num + 1} to S3...")
                try:
                    with NamedTemporaryFile(
                        suffix=".csv", delete=False
                    ) as temp_file:  # delete=Falseë¡œ ì„¤ì •
                        final_expanded_df.to_csv(temp_file.name, index=True)
                        # S3ì— ì—…ë¡œë“œ
                        upload_to_s3(
                            temp_file.name,
                            TRANSFORMED_MATCH_BUCKET,
                            f"transformed_match_data_batch_{batch_num}",
                            file_type="csv",
                        )
                except Exception as e:
                    logging.error(f"ğŸš¨ Error during file operation: {e}")
                finally:
                    if os.path.exists(temp_file.name):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        os.unlink(temp_file.name)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ

                del transformed_dataframe
                del merged_dataframe
                gc.collect()  # ë©”ëª¨ë¦¬ ì ˆì•½ - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
            logging.info("âœ”ï¸ Data upload complete.")

    @task()
    def transform_mastery_data():
        from utils.common_util import download_from_s3, upload_to_s3

        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logging.info("ğŸ“¥ Downloading parquet files from S3...")
        parquet_files = download_from_s3("mastery")

        # load champion mapping data
        script_path = os.path.dirname(os.path.abspath(__file__))
        json_file_path = os.path.join(script_path, "utils", "champion_dictionary.json")
        with open(json_file_path, "r") as f:
            champion_dict = json.load(f)

        if parquet_files:
            logging.info(f"ğŸ” Found {len(parquet_files)} parquet files.")

            for batch_num, batch_files in enumerate(batch(parquet_files, 10)):
                logging.info(f"ğŸ”„ Merging parquet batch {batch_num + 1}...")
                merged_dataframe = _merge_parquet_files(batch_files)

                # ë°ì´í„° ë³€í™˜
                logging.info("ğŸ”„ Transforming data...")
                transformed_dataframe = _create_total_dataframe(
                    merged_dataframe, champion_dict
                )
                logging.info(
                    f"âœ”ï¸ Transformed dataframe shape: {transformed_dataframe.shape}"
                )

                logging.info(f"ğŸ“¤ Uploading batch {batch_num + 1} to S3...")
                try:
                    with NamedTemporaryFile(
                        suffix=".csv", delete=False
                    ) as temp_file:  # delete=Falseë¡œ ì„¤ì •
                        transformed_dataframe.to_csv(temp_file.name, index=True)
                        # S3ì— ì—…ë¡œë“œ
                        upload_to_s3(
                            temp_file.name,
                            TRANSFORMED_MASTERY_BUCKET,
                            f"transformed_mastery_data_chunk_{batch_num + 1}",
                            file_type="csv",
                        )

                except Exception as e:
                    logging.error(f"ğŸš¨ Error during file operation: {e}")
                finally:
                    if os.path.exists(temp_file.name):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        os.unlink(temp_file.name)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ

                del transformed_dataframe
                del merged_dataframe
                gc.collect()  # ë©”ëª¨ë¦¬ ì ˆì•½ - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰

            logging.info("âœ”ï¸ Data upload complete.")

    start = EmptyOperator(task_id="start")

    slack_alert = SlackAlert(channel="#lulu-airflow-alert")

    wait_for_get_riot_api_dag = ExternalTaskSensor(
        task_id="wait_for_get_riot_api_dag",
        external_dag_id="get_riot_api",
        external_task_id="end",
        timeout=600,
        mode="poke",
        dag=dag,
    )

    transform_match_end = EmptyOperator(
        task_id="transform_match_end",
        on_success_callback=slack_alert.slack_success_alert,
        on_failure_callback=slack_alert.slack_failure_alert,
    )

    transform_mastery_end = EmptyOperator(
        task_id="transform_mastery_end",
        on_success_callback=slack_alert.slack_success_alert,
        on_failure_callback=slack_alert.slack_failure_alert,
    )

    end = EmptyOperator(task_id="transform_end")

    start >> wait_for_get_riot_api_dag >> transform_match_data() >> transform_match_end
    transform_match_end >> transform_mastery_data() >> transform_mastery_end >> end
